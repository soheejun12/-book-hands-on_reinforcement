{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical reinforcement learning (HRL)\n",
    "- 강화학습은 많은 상태, 액션 공간을 잘 scaling하지 못함 \n",
    "- 차원의 저주에 빠지기 쉬움 \n",
    "\n",
    "### 많은 상태공간, 액션공간을 다루기 위해 큰 문제들을 작은 sub문제들로 계층적으로 분해 \n",
    "#### - 예시 \n",
    "- 목표: 학교에서 집에 가기 \n",
    "    - sub문제\n",
    "        - 교문 나서기\n",
    "        - 택시 잡기 \n",
    "\n",
    "### HRL에서 사용되는 여러 방법 \n",
    "#### - 상태 공간 분해 (state-space decomposition)\n",
    "- 상태공간을 여러 subspace로 분해 \n",
    "- 더 작아진 subspace에서 문제를 해결 \n",
    "- 빠르게 탐험 가능 \n",
    "    - 전체 상태 공간을 탐험하지 않아도 됨 \n",
    "    \n",
    "#### - 상태 추상화 (state-abstraction)\n",
    "- 에이전트는 일부 변수를 무시 \n",
    "    - 현재 상태공간에서 현재 subtask를 달성하는데 관련없는 변수 무시 \n",
    "    \n",
    "#### - 시간적 추상화 (temporal-abstraction)\n",
    "- 액션 sequence와 액션 set들이 그룹화됨 \n",
    "    - 하나의 step을 여러 step으로 나눔 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * MAXQ Value Function Decomposition\n",
    "#### : HRL에서 많이 사용되는 알고리즘 \n",
    "### 가치 함수를 각 subtask들에 해당하는 가치 함수 set으로 분해 \n",
    "#### - 택시 환경 예시 \n",
    "<img src=\"./image/taxi.PNG\">\n",
    "\n",
    "- 목적: 4개의 위치(R,G,Y,B) 중 한 곳에서 승객을 태우고(pick) 다른 위치에 내려주는 것(drop off)\n",
    "- 보상 \n",
    "    - 올바른 곳에 내려주면 +20 \n",
    "    - 올바르지 못한(illegal) pick, drop off -10 \n",
    "    - 매 time step마다 -1 \n",
    "    \n",
    "#### - subtask 분해 \n",
    "<img src=\"./image/maxq_graph.png\" width=500>   \n",
    " \n",
    " \n",
    "- 모든 subtask들이 계층적으로 정렬됨 \n",
    "- node: subtask 또는 기본 액션 \n",
    "- edge: subtask가 호출할 수 있는 child subtask\n",
    "\n",
    "\n",
    "- Navigate(t)\n",
    "    - 현재 위치에서 다른 위치로 이동 \n",
    "    - 원초(primitive) 액션  4개 \n",
    "        - 북, 남, 동, 서 \n",
    "        \n",
    "        \n",
    "- Get\n",
    "    - 현재 위치에서 승객을 태울 위치로 이동 \n",
    "    - 원초 액션 1개 \n",
    "        - Pickup \n",
    "    - subtask 1개 \n",
    "        - Navigate(t)\n",
    "   \n",
    "   \n",
    "- Put \n",
    "    - 현재 위치에서 승객을 내려줄 위치로 이동\n",
    "    - 원초 액션 1개 \n",
    "        - Putdown \n",
    "    - subtask 1개 \n",
    "        - Navigate(t)\n",
    "    \n",
    "    \n",
    "- root \n",
    "    - 전체 task \n",
    "    \n",
    "#### - 수식화 \n",
    "- MDP M은 subtask들의 set으로 분해 \n",
    "    - ($M_0$, $M_1$, $M_2$, ... $M_n$)\n",
    "    - $M_0$: root task \n",
    "    - $M_1$... : subtasks\n",
    "    \n",
    "    \n",
    "- subtask $M_i$는 semi MDP\n",
    "    - 상태 $S_i$\n",
    "    - 액션 $A_i$\n",
    "    - 확률 전이 함수 \n",
    "        - $P^x_y (s', N | s, a)$\n",
    "    - 기대 보상 함수 \n",
    "        - $\\bar{R}(s, a) = V^\\pi(a, s)$\n",
    "            - 이 때, $V^\\pi(a, s)$: 상태 s에서 child task $M_a$의 투사된(projected) 가치함수 \n",
    "            \n",
    "    - 만약, 액션 a가 원초 액션이라면, \n",
    "        - $V^\\pi(a, s)$: 상태s에서 수행한 액션 a의 기대 즉시 보상 \n",
    "        - $V^\\pi(a, s)= \\sum_{s'}P(s'|s,a). R(s'|s,a)$\n",
    "        \n",
    "    - 벨만 방정식에 따라 다시 쓰면, \n",
    "        $$V^\\pi(i, s) = V^\\pi(\\pi_i(s), s) + \\sum_{s',N} P^\\pi_i(s', N|s, \\pi_i(s)) \\gamma^N V^\\pi(i, s')$$\n",
    "        \n",
    "    - 상태-액션 가치 함수 Q \n",
    "        $$Q^\\pi(i, s, a) = V^\\pi(a, s) + \\sum_{s',N}P^\\pi_i(s', N | s, a)\\gamma^NQ^\\pi(i, s', \\pi(s'))$$\n",
    "        \n",
    "    - completion function: subtask $M_i$를 완료하는 기대할인축적보상 \n",
    "         $$C^\\pi(i, s, a) = \\sum_{s', N}P^\\pi_i(s', N | s, a)\\gamma^NQ^\\pi(i, s', \\pi(s'))$$\n",
    "         \n",
    "    - 이에따른, Q함수\n",
    "         $$Q^\\pi(i, s, a) = V^\\pi(a, s) + C^\\pi(i, s, a)$$\n",
    "         \n",
    "    - 재정의된 가치 함수 \n",
    "    $$V^\\pi(i, s) = \\begin{cases}Q^\\pi(i, s, \\pi(s)) \\text{  if i is composite}\\\\\n",
    "                                   \\sum_{s'}P(s' | s, i)R(s', | s, i) \\text{  if i is primitive}\\end{cases}$$\n",
    "            - 가치함수와 완료함수의 합 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse reinforcement learning\n",
    "- 기존 강화학습은 주어진 보상함수를 통해 최적의 정책을 찾는 것 \n",
    "\n",
    "### 최적의 정책은 주어지고 보상함수를 찾는 것 \n",
    "#### - 이유 \n",
    "- 보상함수를 구성하는것은 복잡 \n",
    "- 잘못된 보상함수는 에이전트가 나쁜행동을 하게 만듦\n",
    "\n",
    "#### - 항상 적절한 보상함수를 알 수는 없지만 옳은 정책은 안다 \n",
    "- 옳은 정책: 각 상태에서의 올바른 행동 \n",
    "    - 인간 전문가를 통해 에이전트에 입력 \n",
    "    - 에이전트는 보상함수를 학습하려 함 \n",
    "    \n",
    "### 예시 \n",
    "#### - 걷는 것을 배울 때 \n",
    "- 무한한 행동에 대한 보상함수를 설계하는 것은 어려움 \n",
    "- 대신 전문가가 demonstration(시범, 설명)을 제공하여 에이전트가 보상함수를 학습하도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
