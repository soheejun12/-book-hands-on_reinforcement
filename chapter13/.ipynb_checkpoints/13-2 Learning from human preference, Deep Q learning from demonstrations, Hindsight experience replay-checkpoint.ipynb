{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from human preference \n",
    "#### :인간의 선호로부터 학습 \n",
    "- 강화학습의 주요한 돌파구 \n",
    "- OpenAI와 DeepMind 연구자들에 의해 제안 \n",
    "\n",
    "## - 아이디어 \n",
    "### 인간의 feedback을 통해 에이전트가 학습 \n",
    "<img src=\"./image/preference.png\" width=400>\n",
    "\n",
    "- 1. 에이전트는 random policy를 통해 환경과 상호작용 (random하게 행동)\n",
    "- 2. 환경과 상호작용하는 에이전트의 행동은 2-3초의 video clip 쌍으로 인간에게 전달 \n",
    "- 3. 인간은 video clip을 보고 어떤 것이 더 잘 수행되었는지 에이전트에게 알려줌 \n",
    "- 4. 에이전트는 보상 예측으로부터 그 알림을 받고, 인간의 feedback에 따라 목표와 보상함수를 설정\n",
    "\n",
    "### 궤도(trajectory)는 관찰과 액션의 연속 \n",
    "#### - 궤도 부분(segment)은  $\\sigma$ 로표현 가능 o는 관찰, a는 액션 \n",
    "- 에이전트는 환경으로부터 관찰을 전달받고 어떤 액션을 수행 \n",
    "- 이러한 상호작용의 액션을 두 궤도 부분으로 저장 \n",
    "     - $\\sigma$ = (($o$$_0$, $o$$_0$),($o$$_1$, $o$$_1$),($o$$_2$, $o$$_2$).... ($o$$_k$$_-$$_1$, $o$$_k$$_-$$_1$)) \n",
    "     - 이 두 궤도들이 인간에게 보여짐 \n",
    "     - 인간이 $\\sigma$$_1$ 보다 $\\sigma$$_2$ 를 더 선호한다면,\n",
    "         - 에이전트는 인간이 선호하는 것을 생산해내는 것이 목적이 되고 그에 따른 보상함수를 생성 \n",
    "         - 데이터베이스에 저장 \n",
    "             - ($\\sigma$$_1$, $\\sigma$$_2$, $\\mu$)\n",
    "             - (비선호, 선호, 정책)\n",
    "             - 정책은 선호하는 것을 생산하도록 설정됨 \n",
    "     - 아무것도 선호하지 않으면,\n",
    "         - 데이터베이스에서 제거됨 \n",
    "     - 둘 다 선호한다면, \n",
    "         - $\\mu$ 는 uniform으로 설정됨 \n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=oC7Cw3fu3gU&feature=youtu.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q learning from demonstrations\n",
    "#### : 설명으로부터의 심층 Q학습 \n",
    "- 지금까지 배워왔던 여러 종류의 DQN들은 경험버퍼를 사용 \n",
    "    - 성능 향상을 위한 학습의 시간이 너무 오래 걸림 \n",
    "    - 실제상황에서는 문제가 될 수 있음     \n",
    "- DeepMind에서 DQN에서 발전된 DQfd(deep Q learning  from demonstrations) 개발 \n",
    "\n",
    "### 미리 준비된 demo를 경험 재생 버퍼에 추가 \n",
    "#### - 아타리 게임 예시 \n",
    "- 좋은 상태, 각 상태에서 높은 보상을 주는 액션에대한 demo를 미리 가지고 있다면, \n",
    "    - 즉각적으로, 에이전트는 학습에 demo 사용 가능 \n",
    "- 작은 양의 demo만으로도 성능 향상, 학습 시간 감소 가능 \n",
    "- demo는 우선순위경험재생버퍼에도 바로 추가됨 \n",
    "\n",
    "### 손실함수 \n",
    "#### - 여러 손실들의 합 \n",
    "- demo에 대한 overfitting 방지를 위해 L2 regulaization 손실 사용 \n",
    "- TD loss와 supervised loss 계산 \n",
    "    - 에이전트가 demo로부터 어떻게 학습하는지 알기위해 \n",
    "    \n",
    "### 성능 \n",
    "- 본 논문의 저자는 여러 환경에서 DQfd 실험 \n",
    "    - prioritized dueling Double DQN보다 더 나은 성능과 빠른 속도를 보임 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=4IFZvqBHsFY&feature=youtu.be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindsight experience replay (HER)\n",
    "#### : 경험재생버퍼에 대한 새로운 기법 \n",
    "#### : 실패해도 얻는 것이 있다 \n",
    "- OpenAI 연구자들이 드문 보상을 해결하기위해 개발 \n",
    "\n",
    "### 인간은 실패를 통해 학습 \n",
    "#### : 실패를통해 실제 목표와 다른 목표 학습 \n",
    "#### - 자전거타는 것을 배울 때 \n",
    "- 처음에는 균형을 제대로 잡지 못함\n",
    "- 하지만 실패를 통해 어떻게 하면 균형이 안잡히는지 배움 \n",
    "- 자전거를 타는 것(실제 목표)은 못 배웠지만 균형을 안 잡는 것(다른 목표)을 배움 \n",
    "\n",
    "### 논문 예시 \n",
    "#### - FetchSlide환경 \n",
    "<img src=\"./image/her1.png\" width=300>\n",
    "\n",
    "- 로봇팔을 움직여 퍽을 쳐서 target(빨간 원)을 맞추는 것이 목표 \n",
    "\n",
    "#### - 초반에 에이전트는 -1의 보상만 얻음 \n",
    "- 잘못하고 있으며 목표를 달성하지 못함을 의미 \n",
    "- 하지만 아무것도 학습하지 못한 것은 아님 \n",
    "\n",
    "   \n",
    "#### - 에이전트는 다른 목표(실제 목표에 가까워지는 것)을 배움 \n",
    "\n",
    "<img src=\"./image/her2.png\" width=300>\n",
    "\n",
    "- 실패가 아니라 다른 목표를 학습했다 생각 \n",
    "- 이 과정을 몇 번 반복하다보면 에이전트는 실제 목표를 달성하는 것을 배움 \n",
    "\n",
    "#### - off-policy 알고리즘에 적용 가능 \n",
    "\n",
    "### 성능\n",
    "- DDPG와 HER을 함께 사용한 것이 DDPG만 사용한 것보다 더 좋은 성능을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=Dz_HuzgMxzo&feature=youtu.be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
