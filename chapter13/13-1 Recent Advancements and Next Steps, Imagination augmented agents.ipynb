{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. Recent Advancements and Next Steps\n",
    "    \n",
    "#### 이번 장에서는 \n",
    "- Imagination augmented agents (I2A)\n",
    "- 인간의 선호(preference)으로부터 학습\n",
    "- 설명(demonstrations)으로부터 Deep Q 학습 \n",
    "- 경험 재생(experience replay)에 대한 깨달음 \n",
    "- 계층적(Hierarchical) 강화학습 \n",
    "- 역(Inverse) 강화학습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagination augmented agents \n",
    "#### (상상력이 증가된 에이전트)\n",
    "#### : 액션을 수행하기 전에 결과(consequence)를 생각하고 수행할 실제 액션을 결정 \n",
    "\n",
    "## - 실제상황 예시 \n",
    "### 체스 게임 \n",
    "- 체스 말을 움직일 때, \n",
    "    - 어떤 말을 움직일 때, 그 후에 따를 결과를 생각 \n",
    "    - 그 말을 움직이는 것이 승리에 도움을 주는지 생각 \n",
    "    \n",
    "- 액션을 수행하기 전에 그에 따를 결과를 생각하고 합당하면 그 액션을 수행, 아니면 수행하지 않음 \n",
    "\n",
    "\n",
    "## - imagination augmented 에이전트 \n",
    "- 기존 에이전트에 상상력(imagination)이 증가된 에이전트 \n",
    "- 환경에서 어떤 액션을 취하기 전에 그 액션을 수행했을 때의 결과를 상상하고\n",
    "- 액션이 좋은 보상을 생산하면 그 액션을 수행\n",
    "- 또한 다른 액션을 수행했을 때의 결과도 상상함 \n",
    "\n",
    "\n",
    "## - 작동 방법 \n",
    "<img src =\"./image/I2A.png\" width=200>\n",
    "\n",
    "### model-based 학습과 model-free 학습의 장점을 모두 가짐 \n",
    "#### - 에이전트가 수행하는 액션은 model-based 경로와 model-free 경로의 결과 \n",
    "#### - model-based 경로, \n",
    "- rollout encoder 존재  \n",
    "    - 에이전트가 상상 작업을 수행하는 곳 \n",
    "    - 두 개의 레이어 (imgine future, encoder)\n",
    "            \n",
    "         <img src =\"./image/imagination_rollout.png\" width=300>\n",
    "                \n",
    "     - imagine future \n",
    "          - 상상이 발생하는 곳 \n",
    "          - imgination core를 가지고 있음 \n",
    "          - 상태 $o_t$가 전달되면 새로운 상태 $\\hat{o}_t$_$_+$_$_1$ , 보상 $\\hat{r}_t$_$_+$_$_1$을 얻음 \n",
    "          - 새로운 상태 $\\hat{o}_t$_$_+$_$_1$가 전달되면,  다음 상태 $\\hat{o}_t$_$_+$_$_2$,  보상 $\\hat{r}_t$_$_+$_$_2$을 얻음 \n",
    "          - 이 과정을 n step 반복하면 state, reward 들의 쌍을 얻게됨\n",
    "                    \n",
    "     - encoder\n",
    "          - 이 결과를 encoding 하기위해 LSTM과 같은 encoder를 사용\n",
    "          - 결과적으로 rollout encoding을 얻음 \n",
    "                        \n",
    "     - rollout encoding은 future imaged path를 표현하는 embedding \n",
    "     - 여러 다른 future imaged path에 대해 여러 rollout encodr들을 가짐 \n",
    "     - 이것을 aggregator로 모아서 사용함 \n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "- imagination core \n",
    "       \n",
    "    <img src=\"./image/imagination_core.png\" width=300>\n",
    "       \n",
    "     - 하나의 imagination core는 policy network와 environment model로 구성  \n",
    "         - policy network \n",
    "         - environment model \n",
    "            - 모든 것이 진행되는 곳 \n",
    "            - 에이전트가 수행한 모든 액션으로부터 학습 \n",
    "            - 상태 $\\hat{o}_t$에 대한 정보를 얻고 경험에 기반하여 모든 가능한 미래를 상상하고 가장 높은 보상을 주는 액션 $\\hat{a}_t$ 선택 \n",
    "               \n",
    "               \n",
    "<img src=\"./image/I2A_all.png\">\n",
    "\n",
    "\n",
    "\n",
    "## - 예시 \n",
    "### Sokoban 게임 \n",
    "<img src=\"./image/sokoban.png\" width=300>\n",
    "\n",
    "- 퍼즐 게임 \n",
    "- 박스를 정해진 위치로 미는 것이 목적 \n",
    "- 규칙 \n",
    "    - 박스는 밀 수만 있으며 당길 수는 없음 \n",
    "    - 박스를 잘못된 곳에 밀면 게임을 해결할 수 없어짐 \n",
    "    \n",
    "  \n",
    "- 게임을 풀 때, \n",
    "    - 게임을 끝내는 아쁜 행동을 하기 전에 미리 상상(생각)하고 계획해야함 \n",
    "    - I2A\n",
    "        - 에이전트가 액션을 수행하기 전에 먼저 계획하는 환경에서 좋은 결과를 보임 \n",
    "        - 실제 논문 저자는 이 게임에서 본 모델을 통해 의미있는 결과를 얻음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
