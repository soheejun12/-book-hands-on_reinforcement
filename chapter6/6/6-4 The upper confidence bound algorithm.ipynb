{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The upper confidence bound 알고리즘 \n",
    "#### UCB\n",
    "#### - epsilon greedy, softmax exploration에서는 확률을 통해 랜덤 액션 선택 \n",
    "    - 액션을 랜덤하게 선택하는 것은 여러 레버를 탐험하는 것에는 유용 \n",
    "    - 하지만, 좋은 보상을 얻기는 힘듦\n",
    "    - 또한, 초기에는 성적이 나쁘지만 사실은 좋은 레버를 놓칠수도 있음 \n",
    "    \n",
    "####  이런 점을 해결하는 UCB알고리즘 \n",
    "- 불확실한 낙관주의 ..(optimism in the face of uncertainty) 근거 \n",
    "\n",
    "- 신뢰구간 (confidence interval)을 기반으로 최적의 레버를 선택 \n",
    "    - 신뢰구간 예시)\n",
    "    - 레버1: 0.3  / 레버2: 0.8 의 보상을 줌 \n",
    "    - 이때, 하나의 라운드에서 레버2가 좋은 보상을 주었다고 최적의 레버라 생각하면 안됨 \n",
    "    - 레버를 여러번 당기고 각 레버의 평균보상값이 최대인 레버를 선택해야 함 \n",
    "    - 각 레버의 정확한 평균값을 찾을 때, 신뢰구간이 필요 \n",
    "    - 신뢰구간: 레버의 평균보상값이 있는 구간을 지정 \n",
    "    \n",
    "        - 레버1의 신뢰구간=[0.2, 0.9] : 레버1의 평균보상값이 0.2 ~ 0.9 \n",
    "        - 0.2: 하위 신뢰 경계 \n",
    "        - 0.9: 상위 신뢰 경계 (UCB) \n",
    "        \n",
    "- UCB는 탐험을 위해 높은 UCB를 갖는 슬롯머신을 선택 \n",
    "\n",
    "#### 예시 \n",
    "<img src = \"./image/UCB.PNG\">\n",
    "\n",
    "- 세개의 슬롯머신을 각 10번씩 수행했을 때의 신뢰 구간 \n",
    "- 슬롯머신3이 가장 높은 UCB값을 가짐 \n",
    "    - 10번밖에 수행하지 않았기 때문에 슬롯머신3이 좋은 보상을 제공한다는 결론을 내기에는 섣부름 \n",
    "    - 수행횟수가 증가할수록, 신뢰구간은 더 정확해지고, 좁아지고, 실제값에 가까워짐\n",
    "    \n",
    "<img src = \"./image/UCB2.PNG\">   \n",
    "\n",
    "- 수행횟수를 더 늘림\n",
    "- 이제 UCB값이 가장 높은 슬롯머신2를 선택할 수 있음\n",
    "\n",
    "####  UCB알고리즘 순서 \n",
    "- 1. 평균보상합과 UCB가 높은 액션(레버)를 선택\n",
    "- 2. 레버를 당기고 보상을 얻음 \n",
    "- 3. 레버의 보상과 신뢰구간을 업데이트 \n",
    "\n",
    "#### UCB계산 \n",
    "<img src = \"./image/UCB3.PNG\">   \n",
    "- N(a): 레버가 당겨진 횟수 \n",
    "- t: 전체 라운드(반복) 횟수\n",
    "- 어떤 레버를 당길지 \n",
    "<img src = \"./image/UCB4.PNG\">   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_bandits\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Environment '<class 'gym_bandits.bandit.BanditTenArmedGaussian'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BanditTenArmedGaussian-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UCB함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB(iters):\n",
    "    ucb = np.zeros(10)\n",
    "    \n",
    "    #모든 레버 탐색 \n",
    "    if iters < 10:\n",
    "        return iters #i\n",
    "    \n",
    "    else:\n",
    "        for i in range(10):\n",
    "            \n",
    "            #UCB계산 \n",
    "            upper_bound = math.sqrt((2*math.log(sum(count))) / count[arm])\n",
    "            \n",
    "            #Q값에 UCB값 더하기 \n",
    "            ucb[arm] = Q[arm] + upper_bound\n",
    "            \n",
    "        #최대값을 갖는 레버     \n",
    "        return (np.argmax(ucb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 20000\n",
    "\n",
    "\n",
    "count = np.zeros(10)\n",
    "\n",
    "sum_rewards = np.zeros(10)\n",
    "\n",
    "Q = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal arm is 8\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    \n",
    "    \n",
    "    arm = UCB(i)\n",
    "    \n",
    "   \n",
    "    observation, reward, done, info = env.step(arm) \n",
    "    \n",
    "    \n",
    "    count[arm] += 1\n",
    "    \n",
    "    \n",
    "    sum_rewards[arm]+=reward\n",
    "    \n",
    "    \n",
    "    Q[arm] = sum_rewards[arm]/count[arm]\n",
    "    \n",
    "    \n",
    "    \n",
    "print( 'The optimal arm is {}'.format(np.argmax(Q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
