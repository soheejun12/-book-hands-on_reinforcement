{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAB 문제 \n",
    "#### Multi-Armed Bandit \n",
    "#### - 강화학습의 전형적인 문제 중 하나 \n",
    "\n",
    "### 이번 장에서는, \n",
    "#### - MAB 문제 \n",
    "#### - Epsilon greedy 알고리즘\n",
    "#### - Softmax exploration 알고리즘 \n",
    "#### - Upper confidence bound 알고리즘\n",
    "#### - Thompson sampling 알고리즘 \n",
    "#### - MAB의 적용 \n",
    "#### - MAB를 사용하여 올바를 광고 식별하기 \n",
    "#### - Contextual bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MAB problem \n",
    "\n",
    "\n",
    "### - MAB는 슬롯 머신\n",
    "- 랜덤하게 생성된 확률 분포를 기반으로 팔(레버)를 당기면 돈(보상)이 나옴 \n",
    "- One-armed bandit: 하나의 슬롯머신이 있을 경우 \n",
    "- multi/k-armed bandits: 여러개의 슬롯머신이 있을 경우 \n",
    "\n",
    "<img src = \"./image/MAB.PNG\">\n",
    "\n",
    "- 각 슬롯머신은 각 확률 분포에 기반하여 보상을 제공 \n",
    "\n",
    "- 우리의 목적은 시간에 따라 어떤 슬롯머신이 최대의 축적보상을 제공하는지 찾는 것 \n",
    "\n",
    "    - 각 timestep t에 대해, 에이전트는 액션 $a_t$를 수행하고, 레버를 당기면 보상 $r_t$를 받음 \n",
    "    - 에이전트의 목적은 최대의 축적보상을 얻는 것\n",
    "    \n",
    "    \n",
    "- 각 레버의 Q(a)값은 레버를 당김으로써 얻는 보상의 평균 \n",
    "<img src = \"./image/MAB_Q.PNG\">\n",
    "\n",
    "- 최적의 레버는 최대 축적보상 값을 제공하는 것 \n",
    "<img src = \"./image/MAB_Q2.PNG\">\n",
    "\n",
    "- 에이전트의 목적은 최적의 레버를 찾고 손실(regret)을 최소화 하는 것 \n",
    "    - 비용(cost)이라 정의될 수 있음 \n",
    "    \n",
    "    \n",
    "- 최적의 레버를 찾는 방법 \n",
    "    - 탐험-탐색 딜레마 발생 \n",
    "    - 이 딜레마를 해결할 여러 탐험 정책\n",
    "        - epsilon-greedy 정책\n",
    "        - upper confidence bound 알고리즘\n",
    "        - thomshon sampling 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_bandits #git으로 설치 \n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Environment '<class 'gym_bandits.bandit.BanditTenArmedGaussian'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BanditTenArmedGaussian-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
