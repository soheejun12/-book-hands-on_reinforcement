{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Temporal Diffrence Learning \n",
    "\n",
    "#### 앞 서 살펴본 몬테카를로 방법\n",
    "- MDP(Markov Decision Process)를 해결하기 위한 방법 \n",
    "- 환경을 알지 못할 때 \n",
    "- 가치 함수와 가치 함수를 최적화하는 제어방법을 찾기 위해 사용 \n",
    "- 단점 \n",
    "    - Episodic 작업에만 적용 가능 \n",
    "    - 에피소드가 매우 길 경우, 가치 함수를 계산하기 위해 오래 기다려야 함 \n",
    "    \n",
    "#### 이번장에서 다룰 알고리즘 TD (Temporal-Difference) 알고리즘 \n",
    "- 사전에 모델에 대한 정보를 알 필요가 없음 \n",
    "- non-episodic 작업에 적용 가능 \n",
    "\n",
    "#### 이번 장에서는, \n",
    "- TD 학습\n",
    "- Q 학습\n",
    "- SARSA\n",
    "- Q 학습과 SARSA를 사용한 Taxi scheduling\n",
    "- Q 학습과 SARSA의 차이점 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD learning\n",
    "#### - 1988년 서튼(Sutton) 교수에 의해 소개 \n",
    "#### - 몬테카를로 방법과 동적 프로그래밍의 장점을 모두 가짐 \n",
    "- 모델 정보를 미리 알지 않아도 됨 \n",
    "- 가치함수 평가를 계산하기 위해 에피소드가 끝날 때까지 기다릴필요 없음 \n",
    "\n",
    "\n",
    "#### - 부트스트래핑 (Bootstrapping) 사용 \n",
    "- 이전에 학습한 추정(estimate)에 기반하여 현재 추정을 근사화(approximate)\n",
    "- 몬테카를로 방법에서는 사용하지 않으며 마지막에만 평가를 진행 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD prediction\n",
    "### * 몬테카를로 방법과 같이 상태가치를 예측 \n",
    "몬테카를로 방법에서는, 총보상(return)의 평균값을 통해 가치함수를 추정 \n",
    "\n",
    "TD 학습은, 이전 상태를 현재 상태를 통해 업데이트 \n",
    "#### - TD 업데이트 규칙에 따라 상태의 가치 업데이트 \n",
    "<img src=\"./image/TD_UPDATE.PNG\">\n",
    "이전 상태의 가치 = 이전 상태의 가치 + 학습률(보상 + 할인계수(현재 상태의 가치) - 이전 상태의 가치) \n",
    "<img src=\"./image/TD_UPDATE2.PNG\">\n",
    "\n",
    "    - 학습률 (파랑)\n",
    "\n",
    "    - 실질적인 실제 보상 (빨강)\n",
    "\n",
    "    - 기대 보상 (노랑)\n",
    "- 갱신되는 이전 상태는 실질적인 실제 보상과 기대보상 간의 차이에 학습률 알파를 곱한 것 \n",
    "\n",
    "#### - TD 에러 \n",
    "<img src=\"./image/TD_UPDATE3.PNG\">\n",
    "\n",
    "- 실질적인 실제보상과 기대 보상의 차이 \n",
    "- 몇 번의 반복을 통해 이 에러를 최소화하려 시도 \n",
    "\n",
    "### * TD prediction 알고리즘 스텝 순서 \n",
    "- 1. V(S)를 0 또는 임의의 값으로 초기화 \n",
    "- 2. 에피소드를 시작하고 각 에피소드의 스텝마다 상태 S에서 액션 A를 수행하고 보상 R을 받고 다음 상태 S'로 이동 \n",
    "- 3. TD 업데이트 규칙을 사용하여 이전 상태 가치를 업데이트 \n",
    "- 4. 종료 상태 (terminal state)에 도달할 때까지 단계 3과 4를 반복?(2, 3)\n",
    "\n",
    "### * FROZEN LAKE 환경 예시  \n",
    "<img src=\"./image/FROZEN_LAKE.PNG\">\n",
    "\n",
    "- 가정 \n",
    "    - 가치 함수 0으로 초기화\n",
    "    - S (1, 1)에서 시작 \n",
    "    - 'right' 액션을 통해 S'인 (1, 2)로 이동하면 보상 -0.3을 얻음 \n",
    "    \n",
    "    \n",
    "- 이러한 정보를 토대로 가치를 업데이트 해보자\n",
    "    - 학습률 = 0.1 \n",
    "    - 할인 계수 = 0.5\n",
    "    - 현재 상태 가치 = 0\n",
    "    - 다음 상태 가치 = 0\n",
    "    - 얻을 수 있는 보상 = -0.3\n",
    "    <img src=\"./image/TD_UPDATE.PNG\">\n",
    "    <img src=\"./image/FROZEN_LAKE_TD.PNG\">\n",
    "    <img src=\"./image/FROZEN_LAKE_TD_UPDATE.PNG\">\n",
    "    \n",
    "- 현재 상태 S는 (1, 2)가 됨 \n",
    "    - 'right' 액션을 통해 S'인 (1, 3)으로 이동하면 보상 -0.3을 얻음 \n",
    "    - 이 때, (1, 2)의 상태를 업데이트 해보자 \n",
    "    - 학습률 = 0.1 \n",
    "    - 할인 계수 = 0.5\n",
    "    - 현재 상태 가치 = 0\n",
    "    - 다음 상태 가치 = 0\n",
    "    - 얻을 수 있는 보상 = -0.3\n",
    "    <img src=\"./image/FROZEN_LAKE_TD2.PNG\">\n",
    "    <img src=\"./image/FROZEN_LAKE_TD_UPDATE2.PNG\">\n",
    "    \n",
    "- 현재 상태 S는 (1, 3)가 됨 \n",
    "    - 'left' 액션을 통해 S'인 (1, 2)로 되돌아가면 보상 -0.3을 얻음 \n",
    "    - 이 때, (1, 3)의 상태를 업데이트 해보자 \n",
    "    - 학습률 = 0.1 \n",
    "    - 할인 계수 = 0.5\n",
    "    - 현재 상태 가치 = 0\n",
    "    - 다음 상태 가치 = -0.03 \n",
    "    - 얻을 수 있는 보상 = -0.3\n",
    "    <img src=\"./image/FROZEN_LAKE_TD3.PNG\">\n",
    "    <img src=\"./image/FROZEN_LAKE_TD_UPDATE3.PNG\">\n",
    "   \n",
    "- 이런 식으로 TD 업데이트 규칙을 통해 모든 상태의 가치를 업데이트 할 수 있음 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD control\n",
    "TD prediction에서는 가치함수를 추정 \n",
    "\n",
    "TD control에서는 가치함수를 최적화\n",
    "\n",
    "\n",
    "두가지 종류의 제어 알고리즘 \n",
    "- Q 학습: off-policy 학습 알고리즘 \n",
    "- SARSA: on-policy 학습 알고리즘\n",
    "\n",
    "\n",
    "### * Q learning\n",
    "#### - 매우 간단하고 널리 사용되는 TD 알고리즘 \n",
    "### * (상태, 액션) 쌍을 고려 \n",
    "- 상태 S에서 액션 A를 했을 때의 영향 \n",
    "- 제어 알고리즘에서 상태 가치는 고려하지 않음 \n",
    "\n",
    "<img src =\"./image/Q.PNG\">\n",
    "- TD prediction과 비슷하지만 약간 다름 \n",
    "\n",
    "### * Q learning 알고리즘 순서 \n",
    "- 1. Q함수를 임의의 값으로 초기화 \n",
    "- 2. epsilon-greedy 정책을 통해 상태에 대한 액션을 수행하여 새로운 상태로 이동 \n",
    "- 3. 다음 규칙을 통해 이전 상태의 Q 값 업데이트 \n",
    "<img src =\"./image/Q.PNG\">\n",
    "- 4. 종료상태에 도달할 때까지 단계 2와 4를 반복 \n",
    "\n",
    "<img src =\"./image/FLOW_CHART_Q.PNG\">\n",
    "\n",
    "### * FROZEN LAKE 환경 예시 \n",
    "<img src =\"./image/FROZEN_LAKE_Q.PNG\">\n",
    "\n",
    "- 이전과 같은 frozen lake 환경 \n",
    "    - 현재 상태는 (3, 2)\n",
    "    - 가능한 액션은 'left', 'right'\n",
    "\n",
    "\n",
    "- Q학습에서는 epsilon-greedy 정책을 통해 액션 선택 \n",
    "    - epsilon의 확률로 새로운 액션 선택: 탐험 (explore)\n",
    "    - (1 - epsilon)의 확률로 최적의 액션 선택: 탐색 (exploit)\n",
    "    \n",
    "    \n",
    "- 가정, \n",
    "    - epsilon의 확률로 새로운 액션 'down' 탐험 및 선택 \n",
    "    - 이를 통해, 상태 (3, 2)에서 새로운 상태 (4, 2)로 이동 \n",
    "    <img src =\"./image/FROZEN_LAKE_Q2.PNG\">\n",
    "\n",
    "\n",
    "- 이제 이전 상태 (3, 2)의 가치를 업데이트 해보자 \n",
    " <img src =\"./image/FROZEN_LAKE_Q3.PNG\">\n",
    " \n",
    "    - 학습률 = 0.1\n",
    "    - 할인 계수 = 1\n",
    "    <img src =\"./image/Q.PNG\">\n",
    "    \n",
    "Q((3, 2), down) = Q((3, 2), down) +0.1 * (0.3 + 1 * max[Q((4, 2), action)] - Q((3, 2), down)\n",
    "                  \n",
    "                  \n",
    "                = 0.8 + 0.1 * (0.3 + 1 * max[0.3, 0.5, 0.8] - 0.8)\n",
    "                \n",
    "                = 0.8 + 0.1 * (0.3 + 1*(0.8) - 0.8) \n",
    "                \n",
    "                = 0.83\n",
    "                \n",
    "- ** 주의 **\n",
    "    - 액션을 선택할 때, epsilon greedy 정책 사용 \n",
    "    - Q값을 업데이트 할 때는, epsilon greedy 정책을 사용하지 않고 가장 큰 가치를 가지는 액션을 선택 \n",
    "    \n",
    "    \n",
    "- 이제 현재 상태 (4, 2)\n",
    "    <img src =\"./image/FROZEN_LAKE_Q4.PNG\">\n",
    "    \n",
    "    - (1 - epsilon)의 확률로 최적의 액션인 'right' 선택 (탐색)\n",
    "    \n",
    "\n",
    "- 이제 현재 상태는 (4, 3), 이전 상태 (4, 2)의 가치를 업데이트 해보자 \n",
    "\n",
    "Q((4, 2), right) = Q((4, 2), right) + 0.1 * (0.3 + 1 * max [Q((4, 3), action)] - Q((4, 2), right)\n",
    "                    \n",
    "                 = 0.8 + 0.1 * (0.3 + 1 * max[0.1, 0.3] - 0.8) \n",
    "                 \n",
    "                 = 0.8 + 0.1 * (0.3 + 1 * (0.3) - 0.8) \n",
    "                 = 0.78\n",
    "                 \n",
    "                 \n",
    "#### 즉, Q학습은 취할 액션을 선택할 때 epsilon greedy 정책을 사용하고 Q값을 업데이트 할 때는 최대 Q값을 가지는 액션을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
