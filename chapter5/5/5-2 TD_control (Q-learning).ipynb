{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD control\n",
    "TD prediction에서는 가치함수를 추정 \n",
    "\n",
    "TD control에서는 가치함수를 최적화\n",
    "\n",
    "\n",
    "두가지 종류의 제어 알고리즘 \n",
    "- Q 학습: off-policy 학습 알고리즘 \n",
    "- SARSA: on-policy 학습 알고리즘\n",
    "\n",
    "\n",
    "### * Q learning\n",
    "#### - 매우 간단하고 널리 사용되는 TD 알고리즘 \n",
    "### * (상태, 액션) 쌍을 고려 \n",
    "- 상태 S에서 액션 A를 했을 때의 영향 \n",
    "- 제어 알고리즘에서 상태 가치는 고려하지 않음 \n",
    "\n",
    "<img src =\"./image/Q.PNG\">\n",
    "- TD prediction과 비슷하지만 약간 다름 \n",
    "\n",
    "### * Q learning 알고리즘 순서 \n",
    "<img src =\"./image/FLOW_CHART_Q.PNG\">\n",
    "\n",
    "- 1. Q함수를 임의의 값으로 초기화 \n",
    "- 2. epsilon-greedy 정책을 통해 상태에 대한 액션을 수행하여 새로운 상태로 이동 \n",
    "- 3. 다음 규칙을 통해 이전 상태의 Q 값 업데이트 \n",
    "<img src =\"./image/Q.PNG\">\n",
    "- 4. 종료상태에 도달할 때까지 단계 2와 4를 반복 \n",
    "\n",
    "\n",
    "\n",
    "### * FROZEN LAKE 환경 예시 \n",
    "<img src =\"./image/FROZEN_LAKE_Q.PNG\">\n",
    "\n",
    "- 이전과 같은 frozen lake 환경 \n",
    "    - 현재 상태는 (3, 2)\n",
    "    - 가능한 액션은 'left', 'right'\n",
    "\n",
    "\n",
    "- Q학습에서는 epsilon-greedy 정책을 통해 액션 선택 \n",
    "    - epsilon의 확률로 새로운 액션 선택: 탐험 (explore)\n",
    "    - (1 - epsilon)의 확률로 최적의 액션 선택: 탐색 (exploit)\n",
    "    \n",
    "    \n",
    "- 가정, \n",
    "    - epsilon의 확률로 새로운 액션 'down' 탐험 및 선택 \n",
    "    - 이를 통해, 상태 (3, 2)에서 새로운 상태 (4, 2)로 이동 \n",
    "    <img src =\"./image/FROZEN_LAKE_Q2.PNG\">\n",
    "\n",
    "\n",
    "- 이제 이전 상태 (3, 2)의 가치를 업데이트 해보자 \n",
    " <img src =\"./image/FROZEN_LAKE_Q3.PNG\">\n",
    " \n",
    "    - 학습률 = 0.1\n",
    "    - 할인 계수 = 1\n",
    "    <img src =\"./image/Q.PNG\">\n",
    "    \n",
    "Q((3, 2), down) = Q((3, 2), down) +0.1 * (0.3 + 1 * max[Q((4, 2), action)] - Q((3, 2), down)\n",
    "                  \n",
    "                  \n",
    "                = 0.8 + 0.1 * (0.3 + 1 * max[0.3, 0.5, 0.8] - 0.8)\n",
    "                \n",
    "                = 0.8 + 0.1 * (0.3 + 1*(0.8) - 0.8) \n",
    "                \n",
    "                = 0.83\n",
    "                \n",
    "- ** 주의 **\n",
    "    - 액션을 선택할 때, epsilon greedy 정책 사용 \n",
    "    - Q값을 업데이트 할 때는, epsilon greedy 정책을 사용하지 않고 가장 큰 가치를 가지는 액션을 선택 \n",
    "    \n",
    "    \n",
    "- 이제 현재 상태 (4, 2)\n",
    "    <img src =\"./image/FROZEN_LAKE_Q4.PNG\">\n",
    "    \n",
    "    - (1 - epsilon)의 확률로 최적의 액션인 'right' 선택 (탐색)\n",
    "    \n",
    "\n",
    "- 이제 현재 상태는 (4, 3), 이전 상태 (4, 2)의 가치를 업데이트 해보자 \n",
    "\n",
    "Q((4, 2), right) = Q((4, 2), right) + 0.1 * (0.3 + 1 * max [Q((4, 3), action)] - Q((4, 2), right)\n",
    "                    \n",
    "                 = 0.8 + 0.1 * (0.3 + 1 * max[0.1, 0.3] - 0.8) \n",
    "                 \n",
    "                 = 0.8 + 0.1 * (0.3 + 1 * (0.3) - 0.8) \n",
    "                 = 0.78\n",
    "                 \n",
    "                 \n",
    "#### 즉, Q학습은 취할 액션을 선택할 때 epsilon greedy 정책을 사용하고 Q값을 업데이트 할 때는 최대 Q값을 가지는 액션을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
