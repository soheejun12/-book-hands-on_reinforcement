{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doom with DRQN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from vizdoom import *\n",
    "import timeit\n",
    "import math\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_shape(Image,Filter,Stride):\n",
    "    layer1 = math.ceil(((Image - Filter + 1) / Stride))\n",
    "    \n",
    "    o1 = math.ceil((layer1 / Stride))\n",
    "    \n",
    "    layer2 = math.ceil(((o1 - Filter + 1) / Stride))\n",
    "    \n",
    "    o2 = math.ceil((layer2 / Stride))\n",
    "    \n",
    "    layer3 = math.ceil(((o2 - Filter + 1) / Stride))\n",
    "    \n",
    "    o3 = math.ceil((layer3  / Stride))\n",
    "\n",
    "    return int(o3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_input_shape(256, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN():\n",
    "    def __init__(self, input_shape, num_actions, inital_learning_rate):\n",
    "        \n",
    "        \"\"\"hyperparameter 초기화\"\"\"\n",
    "\n",
    "        self.tfcast_type = tf.float32\n",
    "        \n",
    "        #입력 이미지 모양 (높이, 넓이, 채널)\n",
    "        self.input_shape = input_shape  \n",
    "        \n",
    "        #액션 개수 \n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        #학습률 \n",
    "        self.learning_rate = inital_learning_rate\n",
    "                \n",
    "        \n",
    "        \"\"\"CNN hyperparameter\"\"\"\n",
    "        \n",
    "        #필터 크기  \n",
    "        self.filter_size = 5\n",
    "        \n",
    "        #필터 개수 \n",
    "        self.num_filters = [16, 32, 64]\n",
    "        \n",
    "        #stride 크기 \n",
    "        self.stride = 2\n",
    "        \n",
    "        #pool 크기 \n",
    "        self.poolsize = 2        \n",
    "        \n",
    "        #convolutional layer 모양 ///\n",
    "        self.convolution_shape = get_input_shape(input_shape[0], self.filter_size, self.stride) * get_input_shape(input_shape[1], self.filter_size, self.stride) *self.num_filters[2]\n",
    "        \n",
    "        \"\"\"RNN hyperparameter\"\"\"\n",
    "        \n",
    "        #cell 뉴런 개수 \n",
    "        self.cell_size = 100\n",
    "        \n",
    "        #은닉층 개수 \n",
    "        self.hidden_layer = 50\n",
    "        \n",
    "        #dropout 확률 \n",
    "        self.dropout_probability = [0.3, 0.2]\n",
    "\n",
    "        #최적화 관련 hyperparameter\n",
    "        self.loss_decay_rate = 0.96\n",
    "        self.loss_decay_steps = 180\n",
    "\n",
    "        \n",
    "        \"\"\"CNN 변수 초기화\"\"\"\n",
    "\n",
    "        #입력 이미지 (높이, 넓이, 채널)\n",
    "        self.input = tf.placeholder(shape = (self.input_shape[0], self.input_shape[1], self.input_shape[2]), dtype = self.tfcast_type)\n",
    "        \n",
    "        #타겟 벡터 (액션 수, 1)\n",
    "        self.target_vector = tf.placeholder(shape = (self.num_actions, 1), dtype = self.tfcast_type)\n",
    "\n",
    "        #각 필터에 상응하는 특징맵 \n",
    "        self.features1 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, input_shape[2], self.num_filters[0]),\n",
    "                                     dtype = self.tfcast_type) #(5, 5, 3, 16)\n",
    "        \n",
    "        self.features2 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, self.num_filters[0], self.num_filters[1]),\n",
    "                                     dtype = self.tfcast_type) #(5, 5, 16, 32)\n",
    "                                     \n",
    "        \n",
    "        self.features3 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, self.num_filters[1], self.num_filters[2]),\n",
    "                                     dtype = self.tfcast_type) #(5, 5, 32, 64)\n",
    "\n",
    "        \n",
    "        \"\"\"RNN 변수 초기화\"\"\"\n",
    "        \n",
    "        #\n",
    "        self.h = tf.Variable(initial_value = np.zeros((1, self.cell_size)), dtype = self.tfcast_type)\n",
    "        \n",
    "        #(입력-은닉) 가중치 \n",
    "        self.rW = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (self.convolution_shape + self.cell_size)),\n",
    "                                            high = np.sqrt(6. / (self.convolution_shape + self.cell_size)),\n",
    "                                            size = (self.convolution_shape, self.cell_size)),\n",
    "                              dtype = self.tfcast_type)\n",
    "        \n",
    "        #(은닉-은닉) 가중치 \n",
    "        self.rU = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            high = np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            size = (self.cell_size, self.cell_size)),\n",
    "                              dtype = self.tfcast_type)\n",
    "        \n",
    "        #(은닉-출력)가중치             \n",
    "        self.rV = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            high = np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            size = (self.cell_size, self.cell_size)),\n",
    "                              dtype = self.tfcast_type)\n",
    "        \n",
    "        #bias \n",
    "        self.rb = tf.Variable(initial_value = np.zeros(self.cell_size), dtype = self.tfcast_type)\n",
    "        self.rc = tf.Variable(initial_value = np.zeros(self.cell_size), dtype = self.tfcast_type)\n",
    "\n",
    "        \n",
    "        \"\"\"FC 변수 초기화\"\"\"\n",
    "        \n",
    "        #(rnn 출력-fc) 가중치 \n",
    "        self.fW = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (self.cell_size + self.num_actions)),\n",
    "                                            high = np.sqrt(6. / (self.cell_size + self.num_actions)),\n",
    "                                            size = (self.cell_size, self.num_actions)),\n",
    "                              dtype = self.tfcast_type)\n",
    "                             \n",
    "        #bias\n",
    "        self.fb = tf.Variable(initial_value = np.zeros(self.num_actions), dtype = self.tfcast_type)\n",
    "\n",
    "        #학습률 \n",
    "        self.step_count = tf.Variable(initial_value = 0, dtype = self.tfcast_type)\n",
    "        self.learning_rate = tf.train.exponential_decay(self.learning_rate,       \n",
    "                                                   self.step_count,\n",
    "                                                   self.loss_decay_steps,\n",
    "                                                   self.loss_decay_steps,\n",
    "                                                   staircase = False)\n",
    "        \n",
    "        \n",
    "        \"\"\"Network\"\"\"\n",
    "        \n",
    "        \"\"\"CNN\"\"\"\n",
    "        #첫번째 convolutional layer\n",
    "        self.conv1 = tf.nn.conv2d(input = tf.reshape(self.input, \n",
    "                                                     shape = (1, self.input_shape[0], self.input_shape[1], self.input_shape[2])), \n",
    "                                  filter = self.features1, \n",
    "                                  strides = [1, self.stride, self.stride, 1], \n",
    "                                  padding = \"VALID\")\n",
    "        \n",
    "        self.relu1 = tf.nn.relu(self.conv1)\n",
    "        \n",
    "        self.pool1 = tf.nn.max_pool(self.relu1, \n",
    "                                    ksize = [1, self.poolsize, self.poolsize, 1], \n",
    "                                    strides = [1, self.stride, self.stride, 1], \n",
    "                                    padding = \"SAME\")\n",
    "\n",
    "        #두번째 convolutional layer\n",
    "        self.conv2 = tf.nn.conv2d(input = self.pool1, \n",
    "                                  filter = self.features2, \n",
    "                                  strides = [1, self.stride, self.stride, 1], \n",
    "                                  padding = \"VALID\")\n",
    "        \n",
    "        self.relu2 = tf.nn.relu(self.conv2)\n",
    "        \n",
    "        self.pool2 = tf.nn.max_pool(self.relu2, \n",
    "                                    ksize = [1, self.poolsize, self.poolsize, 1], \n",
    "                                    strides = [1, self.stride, self.stride, 1], padding = \"SAME\")\n",
    "\n",
    "        #세번째 convolutional layer\n",
    "        self.conv3 = tf.nn.conv2d(input = self.pool2, \n",
    "                                  filter = self.features3, \n",
    "                                  strides = [1, self.stride, self.stride, 1], \n",
    "                                  padding = \"VALID\")\n",
    "        \n",
    "        self.relu3 = tf.nn.relu(self.conv3)\n",
    "        \n",
    "        self.pool3 = tf.nn.max_pool(self.relu3, \n",
    "                                    ksize = [1, self.poolsize, self.poolsize, 1], \n",
    "                                    strides = [1, self.stride, self.stride, 1], \n",
    "                                    padding = \"SAME\")\n",
    "\n",
    "        #dropout\n",
    "        self.drop1 = tf.nn.dropout(self.pool3, self.dropout_probability[0])\n",
    "        \n",
    "        #reshape\n",
    "        self.reshaped_input = tf.reshape(self.drop1, shape = [1, -1])\n",
    "\n",
    "        \"\"\"RNN\"\"\"\n",
    "        #CNN의 출력이 입력이 됨 \n",
    "        self.h = tf.tanh(tf.matmul(self.reshaped_input, self.rW) + tf.matmul(self.h, self.rU) + self.rb)\n",
    "        \n",
    "        self.o = tf.nn.softmax(tf.matmul(self.h, self.rV) + self.rc)\n",
    "\n",
    "        #dropout\n",
    "        self.drop2 = tf.nn.dropout(self.o, self.dropout_probability[1])\n",
    "        \n",
    "        \n",
    "        \"\"\"FC\"\"\"\n",
    "        #RNN의 출력이 입력이 됨 \n",
    "        self.output = tf.reshape(tf.matmul(self.drop2, self.fW) + self.fb, shape = [-1, 1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.prediction = tf.argmax(self.output)\n",
    "        \n",
    "        #loss\n",
    "        self.loss = tf.reduce_mean(tf.square(self.target_vector - self.output))\n",
    "        \n",
    "        #optimization\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        \n",
    "        #update\n",
    "        self.gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        self.update = self.optimizer.apply_gradients(self.gradients)\n",
    "\n",
    "        self.parameters = (self.features1, self.features2, self.features3,\n",
    "                           self.rW, self.rU, self.rV, self.rb, self.rc,\n",
    "                           self.fW, self.fb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, buffer_size):\n",
    "        \n",
    "        #전이(transition)를 저장할 버퍼 \n",
    "        self.buffer = []       \n",
    "        \n",
    "        #버퍼 크기 \n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    #버퍼가 꽉 찰 경우, 오래된 경험 순으로 제거 \n",
    "    \n",
    "    def appendToBuffer(self, memory_tuplet):\n",
    "        if len(self.buffer) > self.buffer_size: \n",
    "            for i in range(len(self.buffer) - self.buffer_size):\n",
    "                self.buffer.remove(self.buffer[0]) \n",
    "                \n",
    "        self.buffer.append(memory_tuplet)  \n",
    "        \n",
    "        \n",
    "  \n",
    "    #n개의 전이를 랜덤하게 샘플링 \n",
    "    def sample(self, n):\n",
    "        memories = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            memory_index = np.random.randint(0, len(self.buffer))       \n",
    "            memories.append(self.buffer[memory_index])\n",
    "            \n",
    "        return memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes, episode_length, learning_rate, scenario = \"/home/sohee/openai/ViZDoom/scenarios/deathmatch.cfg\", map_path = 'map02', render = False):\n",
    "  \n",
    "    #할인 계수 (감마)\n",
    "    discount_factor = .99\n",
    "    \n",
    "    #버퍼 내의 경험을 갱신할 빈도 \n",
    "    update_frequency = 5\n",
    "    store_frequency = 50\n",
    "    \n",
    "    #결과 출력 빈도 \n",
    "    print_frequency = 1000\n",
    "\n",
    "    #변수 초기화 \n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    old_q_value = 0\n",
    "\n",
    "    #에피소드 별 보상, 손실 \n",
    "    rewards = []\n",
    "    losses = []\n",
    "\n",
    "   \n",
    "    \"\"\"환경\"\"\"\n",
    "    #게임 환경 초기화\n",
    "    game = DoomGame()\n",
    "    \n",
    "    #시나리오 설정 (시나리오 경로 주의)\n",
    "    game.set_doom_scenario_path(scenario)\n",
    "    \n",
    "   \n",
    "    game.set_doom_map(map_path)\n",
    "\n",
    "   \n",
    "    game.set_screen_resolution(ScreenResolution.RES_256X160)    \n",
    "    game.set_screen_format(ScreenFormat.RGB24)\n",
    "\n",
    "   \n",
    "    game.set_render_hud(False)\n",
    "    game.set_render_minimal_hud(False)\n",
    "    game.set_render_crosshair(False)\n",
    "    game.set_render_weapon(True)\n",
    "    game.set_render_decals(False)\n",
    "    game.set_render_particles(False)\n",
    "    game.set_render_effects_sprites(False)\n",
    "    game.set_render_messages(False)\n",
    "    game.set_render_corpses(False)\n",
    "    game.set_render_screen_flashes(True)\n",
    "\n",
    "    game.add_available_button(Button.MOVE_LEFT)\n",
    "    game.add_available_button(Button.MOVE_RIGHT)\n",
    "    game.add_available_button(Button.TURN_LEFT)\n",
    "    game.add_available_button(Button.TURN_RIGHT)\n",
    "    game.add_available_button(Button.MOVE_FORWARD)\n",
    "    game.add_available_button(Button.MOVE_BACKWARD)\n",
    "    game.add_available_button(Button.ATTACK)\n",
    "    \n",
    "   \n",
    "    # okay,now we will add one more button called delta. The above button will only work \n",
    "    # like a keyboard keys and will have only boolean values. \n",
    "\n",
    "    # so we use delta button which emulates a mouse device which will have positive and negative values\n",
    "    # and it will be useful in environment for exploring\n",
    "    \n",
    "    game.add_available_button(Button.TURN_LEFT_RIGHT_DELTA, 90)\n",
    "    game.add_available_button(Button.LOOK_UP_DOWN_DELTA, 90)\n",
    "\n",
    "    # initialize an array for actions (원핫 인코딩)\n",
    "    actions = np.zeros((game.get_available_buttons_size(), game.get_available_buttons_size()))\n",
    "    count = 0\n",
    "    for i in actions:\n",
    "        i[count] = 1\n",
    "        count += 1\n",
    "    actions = actions.astype(int).tolist()\n",
    "\n",
    "\n",
    "    # then we add the game variables, ammo, health, and killcount\n",
    "    game.add_available_game_variable(GameVariable.AMMO0)\n",
    "    game.add_available_game_variable(GameVariable.HEALTH)\n",
    "    game.add_available_game_variable(GameVariable.KILLCOUNT)\n",
    "\n",
    "    # we set episode_timeout to terminate the episode after some time step\n",
    "    # we also set episode_start_time which is useful for skipping intial events\n",
    "    \n",
    "    game.set_episode_timeout(6 * episode_length)\n",
    "    game.set_episode_start_time(10)\n",
    "    game.set_window_visible(render)\n",
    "    \n",
    "    # we can also enable sound by setting set_sound_enable to true\n",
    "    game.set_sound_enabled(False)\n",
    "\n",
    "    # we set living reward to 0 which the agent for each move it does even though the move is not useful\n",
    "    game.set_living_reward(0)\n",
    "\n",
    "    # doom has different modes such as player, spectator, asynchronous player and asynchronous spectator\n",
    "    \n",
    "    # in spectator mode humans will play and agent will learn from it.\n",
    "    # in player mode, agent actually plays the game, so we use player mode.\n",
    "    \n",
    "    game.set_mode(Mode.PLAYER)\n",
    "\n",
    "    #게임환경 초기화 \n",
    "    game.init()\n",
    "\n",
    "    #메인 네트워크, 타겟 네트워크 \n",
    "    actionDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)\n",
    "    targetDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)\n",
    "    \n",
    "    # 경험 버퍼: 크기 1000\n",
    "    experiences = ExperienceReplay(1000)\n",
    "\n",
    "    #모델 저장 \n",
    "    saver = tf.train.Saver({v.name: v for v in actionDRQN.parameters}, max_to_keep = 1)\n",
    "\n",
    "    \n",
    "    \"\"\"학습\"\"\"\n",
    "    # we initialize variables for sampling and storing transistions from the experience buffer\n",
    "    sample = 5 #샘플링 개수 \n",
    "    store = 50 #저장 개수//\n",
    "   \n",
    "   \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            \n",
    "            game.new_episode()\n",
    "            \n",
    "            for frame in range(episode_length):\n",
    "                \n",
    "                # get the game state\n",
    "                state = game.get_state()\n",
    "                s = state.screen_buffer\n",
    "                \n",
    "                # select the action\n",
    "                a = actionDRQN.prediction.eval(feed_dict = {actionDRQN.input: s})[0]\n",
    "                action = actions[a]\n",
    "                \n",
    "                # perform the action and store the reward\n",
    "                reward = game.make_action(action)\n",
    "                \n",
    "                # update total rewad\n",
    "                total_reward += reward\n",
    "\n",
    "               \n",
    "                # if the episode is over then break\n",
    "                if game.is_episode_finished():\n",
    "                    break\n",
    "                 \n",
    "                # store transistion to our experience buffer\n",
    "                if (frame % store) == 0:\n",
    "                    experiences.appendToBuffer((s, action, reward))\n",
    "\n",
    "                # sample experience form the experience buffer        \n",
    "                if (frame % sample) == 0:\n",
    "                    memory = experiences.sample(1)\n",
    "                    mem_frame = memory[0][0]\n",
    "                    mem_reward = memory[0][2]\n",
    "                    \n",
    "                    \n",
    "                    # now, train the network\n",
    "                    Q1 = actionDRQN.output.eval(feed_dict = {actionDRQN.input: mem_frame})\n",
    "                    Q2 = targetDRQN.output.eval(feed_dict = {targetDRQN.input: mem_frame})\n",
    "\n",
    "                    # set learning rate\n",
    "                    learning_rate = actionDRQN.learning_rate.eval()\n",
    "\n",
    "                    # calculate Q value\n",
    "                    Qtarget = old_q_value + learning_rate * (mem_reward + discount_factor * Q2 - old_q_value)    \n",
    "                    \n",
    "                    # update old Q value\n",
    "                    old_q_value = Qtarget\n",
    "\n",
    "                    # compute Loss\n",
    "                    loss = actionDRQN.loss.eval(feed_dict = {actionDRQN.target_vector: Qtarget, actionDRQN.input: mem_frame})\n",
    "                    \n",
    "                    # update total loss\n",
    "                    total_loss += loss\n",
    "\n",
    "                    # update both networks\n",
    "                    actionDRQN.update.run(feed_dict = {actionDRQN.target_vector: Qtarget, actionDRQN.input: mem_frame})\n",
    "                    targetDRQN.update.run(feed_dict = {targetDRQN.target_vector: Qtarget, targetDRQN.input: mem_frame})\n",
    "\n",
    "            rewards.append((episode, total_reward))\n",
    "            losses.append((episode, total_loss))\n",
    "\n",
    "            \n",
    "            print(\"Episode %d - Reward = %.3f, Loss = %.3f.\" % (episode, total_reward, total_loss))\n",
    "\n",
    "\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = \"/home/sohee/openai/ViZDoom/scenarios/deathmatch.cfg\"\n",
    "map_path = 'map02'\n",
    "num_episodes = 1\n",
    "episode_length = 300\n",
    "learning_rate = 0.01\n",
    "render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "    \n",
    "    #시나리오 설정 (시나리오 경로 주의)\n",
    "game.set_doom_scenario_path(scenario)\n",
    "    \n",
    "   \n",
    "game.set_doom_map(map_path)\n",
    "\n",
    "   \n",
    "game.set_screen_resolution(ScreenResolution.RES_256X160)    \n",
    "game.set_screen_format(ScreenFormat.RGB24)\n",
    "\n",
    "   \n",
    "game.set_render_hud(False)\n",
    "game.set_render_minimal_hud(False)\n",
    "game.set_render_crosshair(False)\n",
    "game.set_render_weapon(True)\n",
    "game.set_render_decals(False)\n",
    "game.set_render_particles(False)\n",
    "game.set_render_effects_sprites(False)\n",
    "game.set_render_messages(False)\n",
    "game.set_render_corpses(False)\n",
    "game.set_render_screen_flashes(True)\n",
    "\n",
    "game.add_available_button(Button.MOVE_LEFT)\n",
    "game.add_available_button(Button.MOVE_RIGHT)\n",
    "game.add_available_button(Button.TURN_LEFT)\n",
    "game.add_available_button(Button.TURN_RIGHT)\n",
    "game.add_available_button(Button.MOVE_FORWARD)\n",
    "game.add_available_button(Button.MOVE_BACKWARD)\n",
    "game.add_available_button(Button.ATTACK)\n",
    "    \n",
    "   \n",
    "    # okay,now we will add one more button called delta. The above button will only work \n",
    "    # like a keyboard keys and will have only boolean values. \n",
    "\n",
    "    # so we use delta button which emulates a mouse device which will have positive and negative values\n",
    "    # and it will be useful in environment for exploring\n",
    "    \n",
    "game.add_available_button(Button.TURN_LEFT_RIGHT_DELTA, 90)\n",
    "game.add_available_button(Button.LOOK_UP_DOWN_DELTA, 90)\n",
    "\n",
    "    # initialize an array for actions (원핫 인코딩)\n",
    "actions = np.zeros((game.get_available_buttons_size(), game.get_available_buttons_size()))\n",
    "count = 0\n",
    "for i in actions:\n",
    "    i[count] = 1\n",
    "    count += 1\n",
    "actions = actions.astype(int).tolist()\n",
    "\n",
    "\n",
    "    # then we add the game variables, ammo, health, and killcount\n",
    "game.add_available_game_variable(GameVariable.AMMO0)\n",
    "game.add_available_game_variable(GameVariable.HEALTH)\n",
    "game.add_available_game_variable(GameVariable.KILLCOUNT)\n",
    "\n",
    "    # we set episode_timeout to terminate the episode after some time step\n",
    "    # we also set episode_start_time which is useful for skipping intial events\n",
    "    \n",
    "game.set_episode_timeout(6 * episode_length)\n",
    "game.set_episode_start_time(10)\n",
    "game.set_window_visible(render)\n",
    "    \n",
    "    # we can also enable sound by setting set_sound_enable to true\n",
    "game.set_sound_enabled(False)\n",
    "\n",
    "    # we set living reward to 0 which the agent for each move it does even though the move is not useful\n",
    "game.set_living_reward(0)\n",
    "\n",
    "    # doom has different modes such as player, spectator, asynchronous player and asynchronous spectator\n",
    "    \n",
    "    # in spectator mode humans will play and agent will learn from it.\n",
    "    # in player mode, agent actually plays the game, so we use player mode.\n",
    "    \n",
    "game.set_mode(Mode.PLAYER)\n",
    "\n",
    "    #게임환경 초기화 \n",
    "game.init()\n",
    "\n",
    "    #메인 네트워크, 타겟 네트워크 \n",
    "actionDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)\n",
    "targetDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionDRQN.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.get_available_buttons_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward = 0.000, Loss = 0.798.\n"
     ]
    }
   ],
   "source": [
    "train(num_episodes = 1, episode_length = 300, learning_rate = 0.01, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e465519a7391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactionDRQN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDRQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_available_buttons_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'game' is not defined"
     ]
    }
   ],
   "source": [
    "actionDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
