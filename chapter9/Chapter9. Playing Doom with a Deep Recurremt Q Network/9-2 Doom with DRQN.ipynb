{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doom with DRQN \n",
    "\n",
    "### - 환경 \n",
    "#### https://github.com/mwydmuch/ViZDoom/blob/master/doc/DoomGame.md\n",
    "#### 시나리오: death_match \n",
    "#### map: map02\n",
    "- 액션 7가지 \n",
    "    - 왼쪽으로 이동 \n",
    "    - 오른쪽으로 이동 \n",
    "    - 왼쪽으로 회전 \n",
    "    - 오른쪽으로 회전 \n",
    "    - 전진\n",
    "    - 후진\n",
    "    - 총 발사 \n",
    "   \n",
    "   \n",
    "- 보상 \n",
    "    - 긍정적 보상: 몬스터 죽이기에 성공 \n",
    "    - 부정적 보상: 목숨을 잃거나, 자살, 총알 소진 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from vizdoom import *\n",
    "import timeit\n",
    "import math\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_input_shape()\n",
    "    - CNN layer를 거친 후 RNN layer에 입력되는 벡터의 크기 \n",
    "    - 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_shape(Image,Filter,Stride):\n",
    "    layer1 = math.ceil(((Image - Filter + 1) / Stride))\n",
    "    \n",
    "    o1 = math.ceil((layer1 / Stride))\n",
    "    \n",
    "    layer2 = math.ceil(((o1 - Filter + 1) / Stride))\n",
    "    \n",
    "    o2 = math.ceil((layer2 / Stride))\n",
    "    \n",
    "    layer3 = math.ceil(((o2 - Filter + 1) / Stride))\n",
    "    \n",
    "    o3 = math.ceil((layer3  / Stride))\n",
    "\n",
    "    return int(o3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "#### 총 3개 층 \n",
    "- cnn layer\n",
    "<img src=\"./img/network_cnn.PNG\">\n",
    "\n",
    "\n",
    "- rnn layer\n",
    "<img src=\"./img/network_rnn.PNG\">\n",
    "\n",
    "\n",
    "- fc layer \n",
    "<img src=\"./img/network_fc.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN():\n",
    "    def __init__(self, input_shape, num_actions, inital_learning_rate):\n",
    "        \n",
    "        \"\"\"hyperparameter 초기화\"\"\"\n",
    "\n",
    "        self.tfcast_type = tf.float32\n",
    "        \n",
    "        #입력 이미지 모양 (높이, 넓이, 채널)\n",
    "        self.input_shape = input_shape  \n",
    "        \n",
    "        #액션 개수 \n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        #학습률 \n",
    "        self.learning_rate = inital_learning_rate\n",
    "                \n",
    "        \n",
    "        \"\"\"CNN hyperparameter\"\"\"\n",
    "        \n",
    "        #필터 크기  \n",
    "        self.filter_size = 5\n",
    "        \n",
    "        #필터 개수 \n",
    "        self.num_filters = [16, 32, 64]\n",
    "        \n",
    "        #stride 크기 \n",
    "        self.stride = 2\n",
    "        \n",
    "        #pool 크기 \n",
    "        self.poolsize = 2        \n",
    "        \n",
    "        #convolutional layer 모양 ///\n",
    "        self.convolution_shape = get_input_shape(input_shape[0], self.filter_size, self.stride) * get_input_shape(input_shape[1], self.filter_size, self.stride) * self.num_filters[2]\n",
    "        \n",
    "        \"\"\"RNN hyperparameter\"\"\"\n",
    "        \n",
    "        #cell 뉴런 개수 \n",
    "        self.cell_size = 100\n",
    "        \n",
    "        #은닉층 개수 \n",
    "        self.hidden_layer = 50\n",
    "        \n",
    "        #dropout 확률 \n",
    "        self.dropout_probability = [0.3, 0.2]\n",
    "\n",
    "        #최적화 관련 hyperparameter\n",
    "        self.loss_decay_rate = 0.96\n",
    "        self.loss_decay_steps = 180\n",
    "\n",
    "        \n",
    "        \"\"\"CNN 변수 초기화\"\"\"\n",
    "\n",
    "        #입력 이미지 (높이, 넓이, 채널)\n",
    "        self.input = tf.placeholder(shape = (self.input_shape[0], self.input_shape[1], self.input_shape[2]), dtype = self.tfcast_type)\n",
    "        \n",
    "        #타겟 벡터 (액션 수, 1)\n",
    "        self.target_vector = tf.placeholder(shape = (self.num_actions, 1), dtype = self.tfcast_type)\n",
    "\n",
    "        #각 필터에 상응하는 특징맵 \n",
    "        self.features1 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, input_shape[2], self.num_filters[0]),\n",
    "                                     dtype = self.tfcast_type) #(5, 5, 3, 16)\n",
    "        \n",
    "        self.features2 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, self.num_filters[0], self.num_filters[1]),\n",
    "                                     dtype = self.tfcast_type) #(5, 5, 16, 32)\n",
    "                                     \n",
    "        \n",
    "        self.features3 = tf.Variable(initial_value = np.random.rand(self.filter_size, self.filter_size, self.num_filters[1], self.num_filters[2]),\n",
    "                                     dtype = self.tfcast_type) #(5, 5, 32, 64)\n",
    "\n",
    "        \n",
    "        \"\"\"RNN 변수 초기화\"\"\"\n",
    "        \n",
    "        #\n",
    "        self.h = tf.Variable(initial_value = np.zeros((1, self.cell_size)), dtype = self.tfcast_type)\n",
    "        \n",
    "        #(입력-은닉) 가중치 \n",
    "        self.rW = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (self.convolution_shape + self.cell_size)),\n",
    "                                            high = np.sqrt(6. / (self.convolution_shape + self.cell_size)),\n",
    "                                            size = (self.convolution_shape, self.cell_size)),\n",
    "                              dtype = self.tfcast_type)\n",
    "        \n",
    "        #(은닉-은닉) 가중치 \n",
    "        self.rU = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            high = np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            size = (self.cell_size, self.cell_size)),\n",
    "                              dtype = self.tfcast_type)\n",
    "        \n",
    "        #(은닉-출력)가중치             \n",
    "        self.rV = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            high = np.sqrt(6. / (2 * self.cell_size)),\n",
    "                                            size = (self.cell_size, self.cell_size)),\n",
    "                              dtype = self.tfcast_type)\n",
    "        \n",
    "        #bias \n",
    "        self.rb = tf.Variable(initial_value = np.zeros(self.cell_size), dtype = self.tfcast_type)\n",
    "        self.rc = tf.Variable(initial_value = np.zeros(self.cell_size), dtype = self.tfcast_type)\n",
    "\n",
    "        \n",
    "        \"\"\"FC 변수 초기화\"\"\"\n",
    "        \n",
    "        #(rnn 출력-fc) 가중치 \n",
    "        self.fW = tf.Variable(initial_value = np.random.uniform(\n",
    "                                            low = -np.sqrt(6. / (self.cell_size + self.num_actions)),\n",
    "                                            high = np.sqrt(6. / (self.cell_size + self.num_actions)),\n",
    "                                            size = (self.cell_size, self.num_actions)),\n",
    "                              dtype = self.tfcast_type)\n",
    "                             \n",
    "        #bias\n",
    "        self.fb = tf.Variable(initial_value = np.zeros(self.num_actions), dtype = self.tfcast_type)\n",
    "\n",
    "        #학습률 \n",
    "        self.step_count = tf.Variable(initial_value = 0, dtype = self.tfcast_type)\n",
    "        self.learning_rate = tf.train.exponential_decay(self.learning_rate,       \n",
    "                                                   self.step_count,\n",
    "                                                   self.loss_decay_steps,\n",
    "                                                   self.loss_decay_steps,\n",
    "                                                   staircase = False)\n",
    "        \n",
    "        \n",
    "        \"\"\"Network\"\"\"\n",
    "        \n",
    "        \"\"\"CNN\"\"\"\n",
    "        #첫번째 convolutional layer\n",
    "        self.conv1 = tf.nn.conv2d(input = tf.reshape(self.input, \n",
    "                                                     shape = (1, self.input_shape[0], self.input_shape[1], self.input_shape[2])), \n",
    "                                  filter = self.features1, \n",
    "                                  strides = [1, self.stride, self.stride, 1], \n",
    "                                  padding = \"VALID\")\n",
    "        \n",
    "        self.relu1 = tf.nn.relu(self.conv1)\n",
    "        \n",
    "        self.pool1 = tf.nn.max_pool(self.relu1, \n",
    "                                    ksize = [1, self.poolsize, self.poolsize, 1], \n",
    "                                    strides = [1, self.stride, self.stride, 1], \n",
    "                                    padding = \"SAME\")\n",
    "\n",
    "        #두번째 convolutional layer\n",
    "        self.conv2 = tf.nn.conv2d(input = self.pool1, \n",
    "                                  filter = self.features2, \n",
    "                                  strides = [1, self.stride, self.stride, 1], \n",
    "                                  padding = \"VALID\")\n",
    "        \n",
    "        self.relu2 = tf.nn.relu(self.conv2)\n",
    "        \n",
    "        self.pool2 = tf.nn.max_pool(self.relu2, \n",
    "                                    ksize = [1, self.poolsize, self.poolsize, 1], \n",
    "                                    strides = [1, self.stride, self.stride, 1], padding = \"SAME\")\n",
    "\n",
    "        #세번째 convolutional layer\n",
    "        self.conv3 = tf.nn.conv2d(input = self.pool2, \n",
    "                                  filter = self.features3, \n",
    "                                  strides = [1, self.stride, self.stride, 1], \n",
    "                                  padding = \"VALID\")\n",
    "        \n",
    "        self.relu3 = tf.nn.relu(self.conv3)\n",
    "        \n",
    "        self.pool3 = tf.nn.max_pool(self.relu3, \n",
    "                                    ksize = [1, self.poolsize, self.poolsize, 1], \n",
    "                                    strides = [1, self.stride, self.stride, 1], \n",
    "                                    padding = \"SAME\")\n",
    "\n",
    "        #dropout\n",
    "        self.drop1 = tf.nn.dropout(self.pool3, self.dropout_probability[0])\n",
    "        \n",
    "        #reshape\n",
    "        self.reshaped_input = tf.reshape(self.drop1, shape = [1, -1])\n",
    "\n",
    "        \"\"\"RNN\"\"\"\n",
    "        #CNN의 출력이 입력이 됨 \n",
    "        self.h = tf.tanh(tf.matmul(self.reshaped_input, self.rW) + tf.matmul(self.h, self.rU) + self.rb)\n",
    "        \n",
    "        self.o = tf.nn.softmax(tf.matmul(self.h, self.rV) + self.rc)\n",
    "\n",
    "        #dropout\n",
    "        self.drop2 = tf.nn.dropout(self.o, self.dropout_probability[1])\n",
    "        \n",
    "        \n",
    "        \"\"\"FC\"\"\"\n",
    "        #RNN의 출력이 입력이 됨 \n",
    "        self.output = tf.reshape(tf.matmul(self.drop2, self.fW) + self.fb, shape = [-1, 1])\n",
    "        \n",
    "        \n",
    "        #네트워크가 선택한 액션 \n",
    "        self.prediction = tf.argmax(self.output)\n",
    "        \n",
    "        #loss\n",
    "        self.loss = tf.reduce_mean(tf.square(self.target_vector - self.output))\n",
    "        \n",
    "        #optimization\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        \n",
    "        #update\n",
    "        self.gradients = self.optimizer.compute_gradients(self.loss)\n",
    "        self.update = self.optimizer.apply_gradients(self.gradients)\n",
    "\n",
    "        self.parameters = (self.features1, self.features2, self.features3,\n",
    "                           self.rW, self.rU, self.rV, self.rb, self.rc,\n",
    "                           self.fW, self.fb)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경험 버퍼 \n",
    "    - (상태, 액션, 보상) \n",
    "    - 경험 저장 및 샘플링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay():\n",
    "    def __init__(self, buffer_size):\n",
    "        \n",
    "        #전이(transition)를 저장할 버퍼 \n",
    "        self.buffer = []       \n",
    "        \n",
    "        #버퍼 크기 \n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    #버퍼가 꽉 찰 경우, 오래된 경험 순으로 제거 \n",
    "    \n",
    "    def appendToBuffer(self, memory_tuplet):\n",
    "        if len(self.buffer) > self.buffer_size: \n",
    "            for i in range(len(self.buffer) - self.buffer_size):\n",
    "                self.buffer.remove(self.buffer[0]) \n",
    "                \n",
    "        self.buffer.append(memory_tuplet)  \n",
    "        \n",
    "        \n",
    "  \n",
    "    #n개의 전이를 랜덤하게 샘플링 \n",
    "    def sample(self, n):\n",
    "        memories = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            memory_index = np.random.randint(0, len(self.buffer))       \n",
    "            memories.append(self.buffer[memory_index])\n",
    "            \n",
    "        return memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 네트워크 학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_episodes, episode_length, learning_rate, scenario = \"/home/sohee/openai/ViZDoom/scenarios/deathmatch.cfg\", map_path = 'map02', render = False):\n",
    "  \n",
    "    #할인 계수 (감마)\n",
    "    discount_factor = .99\n",
    "    \n",
    "    #버퍼 내의 경험을 갱신할 빈도 \n",
    "    update_frequency = 5\n",
    "    store_frequency = 50\n",
    "    \n",
    "    #결과 출력 빈도 \n",
    "    print_frequency = 1000\n",
    "\n",
    "    #변수 초기화 \n",
    "    total_reward = 0\n",
    "    total_loss = 0\n",
    "    old_q_value = 0\n",
    "\n",
    "    #에피소드 별 보상, 손실 \n",
    "    rewards = []\n",
    "    losses = []\n",
    "\n",
    "   \n",
    "    \"\"\"환경\"\"\"\n",
    "    #게임 환경 초기화\n",
    "    game = DoomGame()\n",
    "    \n",
    "    #시나리오 설정 (시나리오 경로 주의)\n",
    "    game.set_doom_scenario_path(scenario)\n",
    "    \n",
    "   \n",
    "    game.set_doom_map(map_path)\n",
    "\n",
    "    #게임화면 설정 \n",
    "    game.set_screen_resolution(ScreenResolution.RES_256X160)    \n",
    "    game.set_screen_format(ScreenFormat.RGB24)\n",
    "\n",
    "   \n",
    "    #게임 화면에 표시되는 효과 \n",
    "    game.set_render_hud(False) #head-up display (계기류의 표시를 전방으로 내비추는 장치)\n",
    "    game.set_render_minimal_hud(False)\n",
    "    game.set_render_crosshair(False) #십자선\n",
    "    game.set_render_weapon(True) #무기\n",
    "    game.set_render_decals(False) #벽 마크(도안)\n",
    "    game.set_render_particles(False) #조각\n",
    "    game.set_render_effects_sprites(False) #효과 (총 연기, 핏자국 등)\n",
    "    game.set_render_messages(False) #kills 등에 대한 메시지 \n",
    "    game.set_render_corpses(False) #액터 시체 \n",
    "    game.set_render_screen_flashes(True) #데미지를 입거나 아이템을 주웠을 때 화면 반짝임 효과 \n",
    "    \n",
    "    #에이전트가 수행할 버튼 (액션)\n",
    "    game.add_available_button(Button.MOVE_LEFT)\n",
    "    game.add_available_button(Button.MOVE_RIGHT)\n",
    "    game.add_available_button(Button.TURN_LEFT)\n",
    "    game.add_available_button(Button.TURN_RIGHT)\n",
    "    game.add_available_button(Button.MOVE_FORWARD)\n",
    "    game.add_available_button(Button.MOVE_BACKWARD)\n",
    "    game.add_available_button(Button.ATTACK)\n",
    "    \n",
    "   \n",
    "    #delta button \n",
    "    #연속적인 버튼 (위 버튼들은 키보트 키 처럼 이산적)\n",
    "    #에이전트가 보는 화면 뷰의 각도(angle)\n",
    "    #환경에 대한 탐험(관찰)가능\n",
    "    game.add_available_button(Button.TURN_LEFT_RIGHT_DELTA, 90)\n",
    "    game.add_available_button(Button.LOOK_UP_DOWN_DELTA, 90)\n",
    "\n",
    "    #액션\n",
    "    #9x9 (원핫 인코딩)\n",
    "    actions = np.zeros((game.get_available_buttons_size(), game.get_available_buttons_size()))\n",
    "    count = 0\n",
    "    for i in actions:\n",
    "        i[count] = 1\n",
    "        count += 1\n",
    "    actions = actions.astype(int).tolist()\n",
    "\n",
    "\n",
    "    #ammo, health, killcount 변수 \n",
    "    game.add_available_game_variable(GameVariable.AMMO0)\n",
    "    game.add_available_game_variable(GameVariable.HEALTH)\n",
    "    game.add_available_game_variable(GameVariable.KILLCOUNT)\n",
    "\n",
    "    #한 에피소드의 최대 time step (이 이상의 time step 진행 시 에피소드 종료)\n",
    "    game.set_episode_timeout(6 * episode_length)\n",
    "    \n",
    "    #에피소드가 다시 시작될 텀 \n",
    "    game.set_episode_start_time(10)\n",
    "    \n",
    "    #컴퓨터 화면에 게임 화면 표시 여부 \n",
    "    game.set_window_visible(render)\n",
    "    \n",
    "    #음향효과 \n",
    "    game.set_sound_enabled(False)\n",
    "\n",
    "    #에이전트가 행동을 하고 살아있을 때 보상 \n",
    "    game.set_living_reward(0)\n",
    "\n",
    "    #둠 게임은 여러 모드 제공 (PLAYER,SPECTATOR, ASYN_PLAYER, ASYN_SPECTATOR)\n",
    "    #Mode.PLAYER: 에이전트가 게임 \n",
    "    #Mode.SPECTATOR: 사람이 게임하고 에이전트가 그것을 배움 \n",
    "    game.set_mode(Mode.PLAYER)\n",
    "\n",
    "    #게임환경 초기화 \n",
    "    game.init()\n",
    "\n",
    "    #메인 네트워크, 타겟 네트워크 \n",
    "    actionDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)\n",
    "    targetDRQN = DRQN((160, 256, 3), game.get_available_buttons_size() - 2, learning_rate)\n",
    "    \n",
    "    # 경험 버퍼: 크기 1000\n",
    "    experiences = ExperienceReplay(1000)\n",
    "\n",
    "    #모델 저장 \n",
    "    saver = tf.train.Saver({v.name: v for v in actionDRQN.parameters}, max_to_keep = 1)\n",
    "\n",
    "    \n",
    "    \"\"\"학습\"\"\"\n",
    "    sample = 5 #샘플링 빈도 (학습)\n",
    "    store = 50 #저장 빈도 \n",
    "   \n",
    "   \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            \n",
    "            game.new_episode()\n",
    "            \n",
    "            for frame in range(episode_length):\n",
    "                \n",
    "                #게임 현재 상태 \n",
    "                state = game.get_state()\n",
    "                \n",
    "                s = state.screen_buffer\n",
    "                \n",
    "                #메인 네트워크를 통해 액션 선택 \n",
    "                a = actionDRQN.prediction.eval(feed_dict = {actionDRQN.input: s})[0]\n",
    "                action = actions[a]\n",
    "                \n",
    "                #선택한 액션 수행 후 보상 반환 \n",
    "                reward = game.make_action(action)\n",
    "                \n",
    "                #보상 갱신 \n",
    "                total_reward += reward\n",
    "\n",
    "               \n",
    "                #에피소드가 종료되면 새로운 에피소드 시작 \n",
    "                if game.is_episode_finished():\n",
    "                    break\n",
    "                 \n",
    "                #50번 마다 경험버퍼에 경험 저장 (상태, 액션, 보상)\n",
    "                if (frame % store) == 0:\n",
    "                    experiences.appendToBuffer((s, action, reward))\n",
    "\n",
    "                #5번마다 경험버퍼에서 경험 샘플링         \n",
    "                if (frame % sample) == 0:\n",
    "                    memory = experiences.sample(1)\n",
    "                    mem_frame = memory[0][0] #상태\n",
    "                    mem_reward = memory[0][2] #보상 \n",
    "                    \n",
    "                    \n",
    "                    #네트워크 학습 \n",
    "                    Q1 = actionDRQN.output.eval(feed_dict = {actionDRQN.input: mem_frame})\n",
    "                    Q2 = targetDRQN.output.eval(feed_dict = {targetDRQN.input: mem_frame})\n",
    "\n",
    "                    #학습률 \n",
    "                    learning_rate = actionDRQN.learning_rate.eval()\n",
    "\n",
    "                    #Q값 계산 (타겟 네트워크의 Q값 사용)\n",
    "                    Qtarget = old_q_value + learning_rate * (mem_reward + discount_factor * Q2 - old_q_value)    \n",
    "                    \n",
    "                    #Q값 갱신 \n",
    "                    old_q_value = Qtarget\n",
    "\n",
    "                    #메인 네트워크의 손실 계산 \n",
    "                    loss = actionDRQN.loss.eval(feed_dict = {actionDRQN.target_vector: Qtarget, actionDRQN.input: mem_frame})\n",
    "                    \n",
    "                    total_loss += loss\n",
    "\n",
    "                    #메인, 타겟 네트워크 갱신 \n",
    "                    actionDRQN.update.run(feed_dict = {actionDRQN.target_vector: Qtarget, actionDRQN.input: mem_frame})\n",
    "                    targetDRQN.update.run(feed_dict = {targetDRQN.target_vector: Qtarget, targetDRQN.input: mem_frame})\n",
    "\n",
    "            rewards.append((episode, total_reward))\n",
    "            losses.append((episode, total_loss))\n",
    "\n",
    "            \n",
    "            print(\"Episode %d - Reward = %.3f, Loss = %.3f.\" % (episode, total_reward, total_loss))\n",
    "\n",
    "\n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에피소드 개수: 10000개 \n",
    "- 에피소드 길이: 300개 \n",
    "- 학습률: 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward = 0.000, Loss = 0.309.\n",
      "Episode 1 - Reward = 0.000, Loss = 0.712.\n"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0027f0b102b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-dd0aa5c015ef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_episodes, episode_length, learning_rate, scenario, map_path, render)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m#선택한 액션 수행 후 보상 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m#보상 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "train(num_episodes = 10000, episode_length = 300, learning_rate = 0.01, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
